# Emotion-Detection-from-Speech

With the advent of virtual assistants in computers, speech recognition has become more and more important. Speech recognition has been around for a lot of years but it still is a field that observes continuous improvements like accent recognition, language identification, real time translation, etc. 

One such area where the field is still wanting of quality is speech recognition in noisy environments. Every virtual assistant suffers the problem of being inaccurate in a crowded, or a loud environment which renders them useless. 

The goal of this project is to enhance human speech from noisy audio clips and detect the mood of the speaker.

The algorithm will classify audio as human speech or not human-speech.

This will be followed by analysing the human speech and preparing frequency and amplitude distributions from the audio clip. Such distributions will be used to predict the mood of the speaker. 

“Emotional prosody refers to the melodic and rhythmic components of speech that listeners use to gain insight into a speaker's emotive disposition.”

Like, for example, studies have shown that sad emotions are produced with a higher pitch, less intensity but more vocal energy (2000 Hz), longer duration with more pauses, and a lower first formant. Such emotional prosody based studies will be the basis for our mood prediction algorithms.
